{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "KEY = 'b0ad6680b01344f38334d267c9bf1da2'\n",
    "\n",
    "post_name = 'jz982@georgetown.edu'\n",
    "post_pass = 'Wustlwa98$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BAKU, Azerbaijan — Negotiators from several of the world’s most powerful economies held a contentious meeting in Saturday’s wee hours as they sought to push through a $300 billion-per-year climate financing deal for poorer nations, two people familiar with the discussions told POLITICO. \n",
      "But the offer failed to break the stalemate at the COP29 summit. Instead, vulnerable countries that call the money a life-or-death deal pushed back hard for much higher sums — and complained about being locked out of the discussions as the summit threatened to unravel. \n",
      "In a different closed-door meeting on Saturday afternoon, which POLITICO listened in on, developing countries asked for $500 billion. \n",
      "\n",
      "\n",
      "“We feel as though we are left with nothing from this COP,” Samoan environment minister Cedric Schuster, representing a bloc of 39 island nations, told other countries. He added, “Is this how we treat the countries with the moral high ground in the process, who stand to lose the most and have already lost so much?”\n",
      "The island nations and the bloc of least-developed countries then walked out of the meeting, leaving the conference on the verge of collapse.\n",
      "European Union climate commissioner Wopke Hoekstra acknowledged: “It is iffy whether we will succeed.”\n",
      "The last COP to end without an agreement was in 2000. If this summit collapses, it will push the financing question into 2025, after Donald Trump’s return to the White House.\n",
      "“The developing countries are now saying that it is better to have no agreement than a bad one,” said one European diplomat, granted anonymity to discuss closed-door talks. “Normally that is true but in this case, with the upcoming [Trump] presidency in the U.S., it should be crucial for them to have an agreement now.\"\n",
      "The talks on the Caspian Sea, originally scheduled to close Friday, are nearing their 14th day, and could soon lack a quorum if enough delegates leave for their scheduled flights back home.\n",
      "\n",
      "\n",
      "The $300 billion offer, outlined in text circulating on Saturday among delegates in Azerbaijan’s Olympic Stadium, was a bump up from a $250 billion-a-year proposal the summit’s organizers had issued on Thursday.\n",
      "The updated offer emerged from an overnight meeting among wealthier or more economically powerful countries that started after 1 a.m. Saturday and finally broke up just before sunrise in Baku, two people familiar with the gathering told POLITICO. There was, according to a European diplomat briefed on the discussions, “lots of shouting.”\n",
      "The Saudis were the lightning rod for anger at the meeting, which featured diplomats from Saudi Arabia, China, the United States, India, the United Kingdom, Brazil and Australia, according to the European diplomat and one other person familiar with the details. Both officials, like others in this story, were granted anonymity to discuss the tense and ongoing negotiations. \n",
      "Saudi Arabia’s representatives said they would not answer questions from the press until the conference had officially ended.\n",
      "The oil-rich kingdom had been smarting for a year, after agreeing during the last conference in Dubai to a text that committed the world to transition away from fossil fuels. During this year’s conference in Baku, the Saudis have obstructed almost every effort to negotiate details of how that transition might actually happen, according to four European diplomats and two from Latin America.\n",
      "After the meeting, the rich nations in the room — the U.S., Australia and those from Europe — had agreed to transfer $300 billion per year by 2035 to poor countries to help them fight climate change and transition to cleaner energy, according to two diplomats from Europe, one from South Africa and one from Latin America. That was around $100 billion per year more than most, in particular the U.S., had hoped to agree on coming into the meeting, said the European diplomat, and another official from the same region. \n",
      "\n",
      "\n",
      "The U.S. State Department has repeatedly declined to confirm the amount it was prepared to offer at the talks. But Biden administration climate envoy John Podesta defended the size of the offer Saturday.\n",
      "“The need is great,” said Podesta, as he was followed by a teeming mass of reporters and heckled by climate activists. “The offer on the table from donor countries is high.”\n",
      "U.K. Energy Secretary Ed Miliband called the new offer “a significant scaling up from the $100 billion” that rich countries had pledged years ago.\n",
      "“In the end, parties will have to decide [on] the deal that is offered and whether it's an acceptable deal or not,” he said Saturday. “Personally, I think we need to move [that] forward by significantly increasing the climate finance available to developing countries. We can both help them and help the world in accelerating the clean energy transition.”\n",
      "Saudi Arabia was the lightning rod for anger during the gathering at Azerbaijan’s Olympic Stadium in Baku. | Sean Gallup/Getty Images\n",
      "U.N. Secretary-General António Guterres had spent much of Friday cajoling wealthy countries to raise the number they were promising. Analysis by the U.N. and others has found the needs of developing nations run into the trillions every year. But private capital can also be used to fill the gap.\n",
      "Speaking to POLITICO, Irish climate minister Eamon Ryan confirmed that wealthy countries had agreed to increase the finance goal and that Saudi Arabia had blocked any discussion on lowering greenhouse gas pollution.\n",
      "\n",
      "\n",
      "Later Saturday, according to Ryan and another person familiar with the discussions, the Azerbaijani government organizers of the conference held meetings with representatives of 38 small island nations and informed them of the contours of what most expect to be a final take-it-or-leave-it deal, set to be published sometime later Saturday.\n",
      "“This is what always the developed world does to us in all multilateral agreements,” said Panama's climate envoy, Juan Carlos Monterrey Gómez. “They push and push and push until the last minute. They get us tired, they get us hungry, they get us dizzy, and then we come to terms with agreements that don't truly represent the needs of our people.” \n",
      "Sitting on a couch in the rapidly emptying conference venue, Belize’s permanent U.N. representative Carlos Fuller said: “For us, this isn’t only about money. It’s about survival. I think that gets forgotten here.”\n",
      "Gómez told reporters: “I’m also listening to ‘Bitch Better Have My Money’ by Rihanna nonstop.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.politico.eu/article/cop-29-poor-countries-climate-deal-300-billion-climate-change-baku-meeting/'\n",
    "\n",
    "scraper = NewsScraper()\n",
    "\n",
    "article = scraper.scrape_politico_article(url)\n",
    "\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsScraper():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def scrape_fox_article(self, url):\n",
    "        # Get the HTML Soup from the url via a GET request\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'class':'article-body'})\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p')\n",
    "\n",
    "            # Loop through each paragraph\n",
    "            for para in paragraphs:\n",
    "                # There are in text ads and other non article related objects that we can remove with .decompose()\n",
    "                for span in para.find_all('span'):\n",
    "                    span.decompose()\n",
    "                for span in para.find_all('strong'):\n",
    "                    span.decompose()\n",
    "                for i in para.find_all('i'):\n",
    "                    i.decompose()\n",
    "                for d in para.find_all('div', {'class':'info'}):\n",
    "                    d.decompose()\n",
    "\n",
    "\n",
    "            # Combine all our article related objects into one text and return. here we also filter out all caps paragraphs\n",
    "            # Which never actually belong to the article text.\n",
    "            full_article_text = '\\n'.join([para.text for para in paragraphs if para.text.upper() != para.text])\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "        else:\n",
    "            print(\"The given URL is not connecting to an article. Double check the URL and inspect the page if necessary.\")\n",
    "            print(url)\n",
    "            return None\n",
    "\n",
    "    def scrape_CNN_article(self, url):\n",
    "        # Get the HTML Soup from the url via a GET request\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'class':'article__content-container'})\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p', {'data-editable':'text'})\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([' '.join(para.text.split()) for para in paragraphs])\n",
    "\n",
    "            # Remove common finishers that sometimes but dont always appear\n",
    "            finishers = ['This story has been updated with additional information.']\n",
    "\n",
    "            for finisher in finishers:\n",
    "                full_article_text = full_article_text.replace(finisher, '')\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "        else:\n",
    "            print(\"The given URL is not connecting to an article. Double check the URL and inspect the page if necessary.\")\n",
    "            print(url)\n",
    "            return None\n",
    "\n",
    "    def scrape_washingtonpost_article(self, url):\n",
    "\n",
    "        try: # Fails if login is required\n",
    "            # Get the HTML Soup from the url via a GET request\n",
    "            response = requests.get(url, timeout=3)\n",
    "            html_content = response.text\n",
    "\n",
    "            print(\"Article is behind paywall, using Selenium with credentials to scrape\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            # We need to log in to Washington Post, so we provide our cookies\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Click on the sign in button\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[@data-qa='sc-account-button']\"))\n",
    "            ).click()\n",
    "\n",
    "            # Insert email username\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"username\"))\n",
    "            ).send_keys(post_name)\n",
    "\n",
    "            # Click on next button to get to password\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[@data-test-id='sign-in-btn']\"))\n",
    "            ).click()\n",
    "\n",
    "            # Insert password\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@id='password']\"))\n",
    "            ).send_keys(post_pass)\n",
    "\n",
    "            # Click to sign in\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[@data-test-id='sign-in-btn']\"))\n",
    "            ).click()\n",
    "            \n",
    "            time.sleep(5)\n",
    "            html_content = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'class':'meteredContent grid-center'})\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p', {'data-el':'text'})\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([' '.join(para.text.split()) for para in paragraphs])\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "        else:\n",
    "            print(\"The given URL is not connecting to an article. Double check the URL and inspect the page if necessary.\")\n",
    "            print(url)\n",
    "            return None\n",
    "        \n",
    "    def scrape_breitbart_article(self, url):\n",
    "        # Get the HTML Soup from the url via a GET request\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'class':'entry-content'})    \n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p')\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([''.join(para.text) for para in paragraphs])\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "        else:\n",
    "            print(\"The given URL is not connecting to an article. Double check the URL and inspect the page if necessary.\")\n",
    "            print(url)\n",
    "            return None\n",
    "        \n",
    "    def scrape_bbc_article(self, url):\n",
    "        # Get the HTML Soup from the url via a GET request\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('article')\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p')\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([''.join(para.text) for para in paragraphs])\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "        else:\n",
    "            print(\"The given URL is not connecting to an article. Double check the URL and inspect the page if necessary.\")\n",
    "            print(url)\n",
    "            return None\n",
    "        \n",
    "    def scrape_reuters_article(self, url):\n",
    "        # Reuters loads with javascript, so we use selenium to scrape \n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "        # WebDriverWait(driver, 10).until(\n",
    "        #         EC.presence_of_element_located((By.XPATH, \"//*[@data-testid='ArticleBody']\"))\n",
    "        #     )\n",
    "        \n",
    "        # Collect the HTML\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # And convert to soup as usual\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'data-testid':'ArticleBody'})\n",
    "        print(soup)\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('div', {'data-testid':lambda x: 'paragraph-' in x})\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([''.join(para.text) for para in paragraphs])\n",
    "\n",
    "            return full_article_text\n",
    "        \n",
    "    def scrape_ap_article(self, url):\n",
    "         # Get the HTML Soup from the url via a GET request\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Filter down to just the article body\n",
    "        article_body = soup.find('div', {'class':'RichTextStoryBody RichTextBody'})\n",
    "\n",
    "        if article_body:\n",
    "            # Segment by paragraph\n",
    "            paragraphs = article_body.find_all('p')\n",
    "\n",
    "            # Rebuild the article from the paragraphs\n",
    "            full_article_text = '\\n'.join([''.join(para.text) for para in paragraphs])\n",
    "\n",
    "            return full_article_text\n",
    "    \n",
    "    def scrape_politico_article(self, url):\n",
    "        # Page loads with JS, so use selenium again\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "\n",
    "        # click the agree to privacy terms button if it appears\n",
    "        try:\n",
    "            WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@title='Agree']\"))\n",
    "            ).click()\n",
    "        except:\n",
    "            WebDriverWait(driver, 4).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@class='sidebar-grid__container']\"))\n",
    "            )\n",
    "\n",
    "        # Collect the HTML\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # And convert to soup as usual\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Segment by paragraph. We do not filter to the article body, as it is split into multiple\n",
    "        # pieces in the HTML, and the paragraphs are unique in their class name\n",
    "        paragraphs = soup.find_all('div', {'class':'sidebar-grid__content article__content'})\n",
    "\n",
    "        # Rebuild the article from the paragraphs\n",
    "        full_article_text = '\\n'.join([''.join(para.text) for para in paragraphs])\n",
    "\n",
    "        return full_article_text\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://newsapi.org/v2/everything?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melection\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# query phrase\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-11-15\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-11-18\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiKey\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mKEY\u001b[49m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomains\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoxnews.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparameters)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KEY' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "parameters = {\n",
    "    'q': 'election', # query phrase\n",
    "    'from': '2024-11-15',\n",
    "    'to': '2024-11-18',\n",
    "    'apiKey': KEY,\n",
    "    'domains':'foxnews.com'\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.foxnews.com/politics/javier-milei-first-world-leader-meet-president-elect-trump-greatest-political-comeback-history'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['articles'][-1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
